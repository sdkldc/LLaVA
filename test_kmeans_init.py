"""
K-means ê¸°ë°˜ ìš”ì•½ í† í° ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

ìš”ì•½ í† í°ì´ K-means clusteringì„ í†µí•´ ì˜¬ë°”ë¥´ê²Œ ì´ˆê¸°í™”ë˜ëŠ”ì§€ ê²€ì¦í•©ë‹ˆë‹¤.
"""

import torch
import torch.nn as nn
from llava.model.kmeans_initializer import (
    compute_distance,
    kmeans_clustering,
    initialize_summary_tokens_with_kmeans
)


def test_compute_distance():
    """ê±°ë¦¬ ê³„ì‚° í•¨ìˆ˜ í…ŒìŠ¤íŠ¸"""
    print("\n" + "="*60)
    print("Test 1: Distance Computation")
    print("="*60)
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    embeddings = torch.randn(100, 64)
    centroids = torch.randn(8, 64)
    
    # Cosine distance
    dist_cosine = compute_distance(embeddings, centroids, metric='cosine')
    print(f"âœ… Cosine distance shape: {dist_cosine.shape} (expected: [100, 8])")
    print(f"   Range: [{dist_cosine.min():.4f}, {dist_cosine.max():.4f}]")
    
    # L2 distance
    dist_l2 = compute_distance(embeddings, centroids, metric='l2')
    print(f"âœ… L2 distance shape: {dist_l2.shape} (expected: [100, 8])")
    print(f"   Range: [{dist_l2.min():.4f}, {dist_l2.max():.4f}]")
    
    # Dot product
    dist_dot = compute_distance(embeddings, centroids, metric='dot')
    print(f"âœ… Dot distance shape: {dist_dot.shape} (expected: [100, 8])")
    print(f"   Range: [{dist_dot.min():.4f}, {dist_dot.max():.4f}]")


def test_kmeans_clustering():
    """K-means clustering í…ŒìŠ¤íŠ¸"""
    print("\n" + "="*60)
    print("Test 2: K-means Clustering")
    print("="*60)
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° (576ê°œ ì´ë¯¸ì§€ í† í°, 1024ì°¨ì›)
    num_tokens = 576
    hidden_size = 1024
    num_clusters = 8
    
    embeddings = torch.randn(num_tokens, hidden_size)
    
    # Cosine metric
    print(f"\nğŸ”§ K-means with cosine metric (n_iter=3)")
    centroids_cosine = kmeans_clustering(
        embeddings, 
        num_clusters=num_clusters,
        metric='cosine',
        n_iter=3
    )
    print(f"âœ… Centroids shape: {centroids_cosine.shape} (expected: [{num_clusters}, {hidden_size}])")
    
    # L2 metric
    print(f"\nğŸ”§ K-means with L2 metric (n_iter=3)")
    centroids_l2 = kmeans_clustering(
        embeddings,
        num_clusters=num_clusters,
        metric='l2',
        n_iter=3
    )
    print(f"âœ… Centroids shape: {centroids_l2.shape} (expected: [{num_clusters}, {hidden_size}])")
    
    # ê° centroidê°€ ì„œë¡œ ë‹¤ë¥¸ì§€ í™•ì¸
    uniqueness_cosine = torch.cdist(centroids_cosine, centroids_cosine, p=2)
    uniqueness_l2 = torch.cdist(centroids_l2, centroids_l2, p=2)
    
    # Diagonalì€ 0ì´ë¯€ë¡œ ì œì™¸í•˜ê³  ìµœì†Œ ê±°ë¦¬ í™•ì¸
    mask = torch.eye(num_clusters, dtype=torch.bool)
    min_dist_cosine = uniqueness_cosine[~mask].min()
    min_dist_l2 = uniqueness_l2[~mask].min()
    
    print(f"\nğŸ“Š Centroid Diversity Check:")
    print(f"   Cosine: Min distance between centroids = {min_dist_cosine:.4f}")
    print(f"   L2: Min distance between centroids = {min_dist_l2:.4f}")
    
    if min_dist_cosine > 0.1 and min_dist_l2 > 0.1:
        print("   âœ… Centroids are sufficiently diverse")
    else:
        print("   âš ï¸ Some centroids may be too similar")


def test_initialize_summary_tokens():
    """ìš”ì•½ í† í° ì´ˆê¸°í™” í†µí•© í…ŒìŠ¤íŠ¸"""
    print("\n" + "="*60)
    print("Test 3: Summary Token Initialization")
    print("="*60)
    
    # ëª¨ì˜ vision features (CLIP output)
    num_image_tokens = 576
    vision_hidden_size = 1024
    llm_hidden_size = 4096
    num_summary_tokens = 8
    
    vision_features = torch.randn(num_image_tokens, vision_hidden_size)
    
    # ëª¨ì˜ MM projector (1024 -> 4096)
    class MockProjector(nn.Module):
        def __init__(self):
            super().__init__()
            self.linear1 = nn.Linear(vision_hidden_size, llm_hidden_size)
            self.gelu = nn.GELU()
            self.linear2 = nn.Linear(llm_hidden_size, llm_hidden_size)
        
        def forward(self, x):
            return self.linear2(self.gelu(self.linear1(x)))
    
    mm_projector = MockProjector()
    
    # K-means ì´ˆê¸°í™”
    print(f"\nğŸ”§ Initializing {num_summary_tokens} summary tokens...")
    summary_init = initialize_summary_tokens_with_kmeans(
        vision_features=vision_features,
        mm_projector=mm_projector,
        num_summary_tokens=num_summary_tokens,
        metric='cosine',
        n_iter=3
    )
    
    print(f"âœ… Summary token init shape: {summary_init.shape}")
    print(f"   Expected: [{num_summary_tokens}, {llm_hidden_size}]")
    
    assert summary_init.shape == (num_summary_tokens, llm_hidden_size), \
        f"Shape mismatch: {summary_init.shape} != ({num_summary_tokens}, {llm_hidden_size})"
    
    # ê° ìš”ì•½ í† í°ì´ ì„œë¡œ ë‹¤ë¥¸ì§€ í™•ì¸
    diversity = torch.cdist(summary_init, summary_init, p=2)
    mask = torch.eye(num_summary_tokens, dtype=torch.bool)
    min_dist = diversity[~mask].min()
    
    print(f"\nğŸ“Š Summary Token Diversity:")
    print(f"   Min distance between tokens = {min_dist:.4f}")
    
    if min_dist > 0.1:
        print("   âœ… Summary tokens are sufficiently diverse")
    else:
        print("   âš ï¸ Some summary tokens may be too similar")


def test_different_metrics():
    """ë‹¤ì–‘í•œ ë©”íŠ¸ë¦­ ë¹„êµ í…ŒìŠ¤íŠ¸"""
    print("\n" + "="*60)
    print("Test 4: Metric Comparison")
    print("="*60)
    
    embeddings = torch.randn(576, 1024)
    num_clusters = 8
    
    metrics = ['cosine', 'l2', 'dot']
    centroids_dict = {}
    
    for metric in metrics:
        print(f"\nğŸ”§ Testing metric: {metric}")
        centroids = kmeans_clustering(
            embeddings,
            num_clusters=num_clusters,
            metric=metric,
            n_iter=3
        )
        centroids_dict[metric] = centroids
        
        # Diversity ì²´í¬
        diversity = torch.cdist(centroids, centroids, p=2)
        mask = torch.eye(num_clusters, dtype=torch.bool)
        min_dist = diversity[~mask].min()
        avg_dist = diversity[~mask].mean()
        
        print(f"   Min distance: {min_dist:.4f}")
        print(f"   Avg distance: {avg_dist:.4f}")
    
    print("\nğŸ“Š Recommendation:")
    print("   - Cosine: Best for normalized embeddings (semantic similarity)")
    print("   - L2: Best for absolute distance")
    print("   - Dot: Fast but less interpretable")


if __name__ == "__main__":
    print("\n" + "="*60)
    print("K-MEANS INITIALIZATION TEST SUITE")
    print("="*60)
    
    # ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ì„¤ì •
    torch.manual_seed(42)
    
    try:
        test_compute_distance()
        test_kmeans_clustering()
        test_initialize_summary_tokens()
        test_different_metrics()
        
        print("\n" + "="*60)
        print("âœ… ALL TESTS PASSED!")
        print("="*60)
        print("\nK-means ì´ˆê¸°í™”ê°€ ì˜¬ë°”ë¥´ê²Œ êµ¬í˜„ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print("í•™ìŠµ ì‹œì‘ ì „ vision encoder ì¶œë ¥ì„ K-meansë¡œ í´ëŸ¬ìŠ¤í„°ë§í•˜ì—¬")
        print("ë‹¤ì–‘í•˜ê³  ìœ ì˜ë¯¸í•œ ìš”ì•½ í† í° ì´ˆê¸°ê°’ì„ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n")
        
    except Exception as e:
        print(f"\nâŒ TEST FAILED: {e}")
        import traceback
        traceback.print_exc()
