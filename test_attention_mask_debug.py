#!/usr/bin/env python3
"""
Attention Mask ë””ë²„ê¹… ìŠ¤í¬ë¦½íŠ¸

ëª©ì :
1. 1ì°¨ forward: causal mask + ìš”ì•½ í† í°ë¼ë¦¬ ì„œë¡œ ì°¸ì¡° ë¶ˆê°€
2. 2ì°¨ forward: ì¼ë°˜ LLaVAì²˜ëŸ¼ causal mask + padding mask, ìš”ì•½ í† í°ë¼ë¦¬ ì°¸ì¡° ê°€ëŠ¥

í…ŒìŠ¤íŠ¸ í•­ëª©:
- create_summary_token_attention_mask í•¨ìˆ˜ì˜ ë§ˆìŠ¤í¬ ìƒì„± ë¡œì§
- 1ì°¨ forwardì—ì„œ attention mask ì ìš©
- 2ì°¨ forwardì—ì„œ ì¼ë°˜ attention mask ì‚¬ìš©
"""

import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path

# LLaVA ëª¨ë“ˆ import
from llava.model.attention_utils import (
    create_summary_token_attention_mask,
    create_summary_token_attention_mask_optimized,
    combine_masks,
    convert_mask_to_additive,
    visualize_attention_mask
)


def test_basic_mask_creation():
    """ê¸°ë³¸ ë§ˆìŠ¤í¬ ìƒì„± í…ŒìŠ¤íŠ¸"""
    print("\n" + "="*80)
    print("í…ŒìŠ¤íŠ¸ 1: ê¸°ë³¸ Attention Mask ìƒì„±")
    print("="*80)
    
    batch_size = 2
    seq_length = 20  # í”„ë¡¬í”„íŠ¸(5) + ì´ë¯¸ì§€(10) + ìš”ì•½í† í°(5)
    prompt_len = 5
    image_len = 10
    summary_len = 5
    summary_start = prompt_len + image_len  # 15
    summary_end = summary_start + summary_len  # 20
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    print(f"\nì„¤ì •:")
    print(f"  - Batch size: {batch_size}")
    print(f"  - ì „ì²´ ì‹œí€€ìŠ¤ ê¸¸ì´: {seq_length}")
    print(f"  - í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {prompt_len} (0~{prompt_len-1})")
    print(f"  - ì´ë¯¸ì§€ í† í° ê¸¸ì´: {image_len} ({prompt_len}~{prompt_len+image_len-1})")
    print(f"  - ìš”ì•½ í† í° ê¸¸ì´: {summary_len} ({summary_start}~{summary_end-1})")
    
    # ê¸°ë³¸ ë²„ì „ í…ŒìŠ¤íŠ¸
    print("\n[ê¸°ë³¸ ë²„ì „ í…ŒìŠ¤íŠ¸]")
    mask_basic = create_summary_token_attention_mask(
        batch_size=batch_size,
        seq_length=seq_length,
        summary_token_positions=(summary_start, summary_end),
        device=device,
        dtype=torch.float32
    )
    
    print(f"  - ë§ˆìŠ¤í¬ shape: {mask_basic.shape}")
    print(f"  - ë§ˆìŠ¤í¬ dtype: {mask_basic.dtype}")
    print(f"  - True(ì°¨ë‹¨) ë¹„ìœ¨: {mask_basic.float().mean().item():.2%}")
    
    # ìµœì í™” ë²„ì „ í…ŒìŠ¤íŠ¸
    print("\n[ìµœì í™” ë²„ì „ í…ŒìŠ¤íŠ¸]")
    mask_optimized = create_summary_token_attention_mask_optimized(
        batch_size=batch_size,
        seq_length=seq_length,
        summary_token_positions=(summary_start, summary_end),
        device=device,
        dtype=torch.float32
    )
    
    print(f"  - ë§ˆìŠ¤í¬ shape: {mask_optimized.shape}")
    print(f"  - ë§ˆìŠ¤í¬ dtype: {mask_optimized.dtype}")
    print(f"  - True(ì°¨ë‹¨) ë¹„ìœ¨: {mask_optimized.float().mean().item():.2%}")
    
    # ë‘ ë²„ì „ì´ ë™ì¼í•œì§€ í™•ì¸
    is_same = torch.all(mask_basic == mask_optimized)
    print(f"\n  - ë‘ ë²„ì „ ì¼ì¹˜ ì—¬ë¶€: {is_same}")
    
    if not is_same:
        print("  âš ï¸ ê²½ê³ : ê¸°ë³¸ ë²„ì „ê³¼ ìµœì í™” ë²„ì „ì˜ ê²°ê³¼ê°€ ë‹¤ë¦…ë‹ˆë‹¤!")
        diff_count = (mask_basic != mask_optimized).sum().item()
        print(f"  - ì°¨ì´ ê°œìˆ˜: {diff_count}")
    
    return mask_basic, mask_optimized, (summary_start, summary_end)


def test_mask_properties(mask, summary_positions):
    """ë§ˆìŠ¤í¬ ì†ì„± ê²€ì¦"""
    print("\n" + "="*80)
    print("í…ŒìŠ¤íŠ¸ 2: Attention Mask ì†ì„± ê²€ì¦")
    print("="*80)
    
    summary_start, summary_end = summary_positions
    seq_length = mask.shape[-1]
    
    # 2Dë¡œ ë³€í™˜ (ì²« ë²ˆì§¸ ë°°ì¹˜ ìƒ˜í”Œ)
    mask_2d = mask[0, 0].cpu()  # [seq_length, seq_length]
    
    print("\n[1] Causal Mask ê²€ì¦ (í•˜ì‚¼ê° í–‰ë ¬)")
    # Upper triangular ë¶€ë¶„ì´ ëª¨ë‘ True(ì°¨ë‹¨)ì¸ì§€ í™•ì¸
    upper_tri = torch.triu(torch.ones_like(mask_2d), diagonal=1).bool()
    is_causal = torch.all(mask_2d[upper_tri] == True)
    print(f"  - Causal mask ìœ ì§€: {is_causal}")
    
    if not is_causal:
        violations = (mask_2d[upper_tri] == False).sum().item()
        print(f"  âš ï¸ ê²½ê³ : Causal mask ìœ„ë°˜ {violations}ê°œ ë°œê²¬!")
    
    print("\n[2] ìš”ì•½ í† í° ê°„ ìƒí˜¸ ì°¸ì¡° ì°¨ë‹¨ ê²€ì¦")
    # ìš”ì•½ í† í°ë¼ë¦¬ëŠ” ì„œë¡œ ì°¸ì¡° ë¶ˆê°€ (diagonal ì œì™¸)
    summary_region = mask_2d[summary_start:summary_end, summary_start:summary_end]
    
    print(f"  - ìš”ì•½ í† í° ì˜ì—­ shape: {summary_region.shape}")
    
    # Diagonalì€ False(ìê¸° ì°¸ì¡° ê°€ëŠ¥), off-diagonalì€ True(ì°¨ë‹¨)
    diagonal_values = torch.diagonal(summary_region)
    off_diagonal_mask = ~torch.eye(summary_region.shape[0], dtype=torch.bool)
    off_diagonal_values = summary_region[off_diagonal_mask]
    
    diagonal_ok = torch.all(diagonal_values == False)
    off_diagonal_ok = torch.all(off_diagonal_values == True)
    
    print(f"  - Diagonal (ìê¸° ì°¸ì¡°): {diagonal_ok} (ëª¨ë‘ Falseì—¬ì•¼ í•¨)")
    print(f"  - Off-diagonal (ìƒí˜¸ ì°¸ì¡°): {off_diagonal_ok} (ëª¨ë‘ Trueì—¬ì•¼ í•¨)")
    
    if not diagonal_ok:
        print(f"  âš ï¸ ê²½ê³ : Diagonalì—ì„œ {(diagonal_values == True).sum().item()}ê°œ ì°¨ë‹¨ë¨!")
    if not off_diagonal_ok:
        print(f"  âš ï¸ ê²½ê³ : Off-diagonalì—ì„œ {(off_diagonal_values == False).sum().item()}ê°œ í—ˆìš©ë¨!")
    
    print("\n[3] ìš”ì•½ í† í°ì˜ í”„ë¡¬í”„íŠ¸/ì´ë¯¸ì§€ ì°¸ì¡° ê°€ëŠ¥ ê²€ì¦")
    # ìš”ì•½ í† í°ì€ ì´ì „ì˜ í”„ë¡¬í”„íŠ¸ì™€ ì´ë¯¸ì§€ í† í°ì„ ì°¸ì¡° ê°€ëŠ¥í•´ì•¼ í•¨
    for i in range(summary_start, summary_end):
        can_attend_to_prompt_and_image = torch.all(mask_2d[i, :summary_start] == False)
        if not can_attend_to_prompt_and_image:
            blocked_count = (mask_2d[i, :summary_start] == True).sum().item()
            print(f"  âš ï¸ ìš”ì•½ í† í° {i}ê°€ í”„ë¡¬í”„íŠ¸/ì´ë¯¸ì§€ì˜ {blocked_count}ê°œ ìœ„ì¹˜ë¥¼ ì°¸ì¡° ëª»í•¨!")
        
    # ëª¨ë“  ìš”ì•½ í† í°ì´ í”„ë¡¬í”„íŠ¸/ì´ë¯¸ì§€ë¥¼ ì°¸ì¡° ê°€ëŠ¥í•œì§€ í™•ì¸
    all_can_attend = torch.all(mask_2d[summary_start:summary_end, :summary_start] == False)
    print(f"  - ëª¨ë“  ìš”ì•½ í† í°ì´ í”„ë¡¬í”„íŠ¸/ì´ë¯¸ì§€ ì°¸ì¡° ê°€ëŠ¥: {all_can_attend}")
    
    print("\n[4] í†µê³„ ì •ë³´")
    total_elements = mask_2d.numel()
    blocked_elements = (mask_2d == True).sum().item()
    allowed_elements = (mask_2d == False).sum().item()
    
    print(f"  - ì „ì²´ ìš”ì†Œ: {total_elements}")
    print(f"  - ì°¨ë‹¨ëœ ìš”ì†Œ (True): {blocked_elements} ({blocked_elements/total_elements:.2%})")
    print(f"  - í—ˆìš©ëœ ìš”ì†Œ (False): {allowed_elements} ({allowed_elements/total_elements:.2%})")
    
    return diagonal_ok and off_diagonal_ok and is_causal and all_can_attend


def test_mask_combination():
    """ë§ˆìŠ¤í¬ ê²°í•© í…ŒìŠ¤íŠ¸ (ì»¤ìŠ¤í…€ + íŒ¨ë”©)"""
    print("\n" + "="*80)
    print("í…ŒìŠ¤íŠ¸ 3: Mask ê²°í•© (ì»¤ìŠ¤í…€ + íŒ¨ë”©)")
    print("="*80)
    
    batch_size = 2
    seq_length = 20
    summary_positions = (15, 20)
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # ì»¤ìŠ¤í…€ ë§ˆìŠ¤í¬ ìƒì„±
    custom_mask = create_summary_token_attention_mask_optimized(
        batch_size=batch_size,
        seq_length=seq_length,
        summary_token_positions=summary_positions,
        device=device
    )
    
    # íŒ¨ë”© ë§ˆìŠ¤í¬ ìƒì„± (ë‘ ë²ˆì§¸ ìƒ˜í”Œì— íŒ¨ë”© ìˆë‹¤ê³  ê°€ì •)
    padding_mask = torch.ones(batch_size, seq_length, dtype=torch.bool, device=device)
    padding_mask[1, 18:] = False  # ë‘ ë²ˆì§¸ ìƒ˜í”Œì˜ ë§ˆì§€ë§‰ 2ê°œ í† í°ì€ íŒ¨ë”©
    
    print(f"\nì„¤ì •:")
    print(f"  - Batch size: {batch_size}")
    print(f"  - ì‹œí€€ìŠ¤ ê¸¸ì´: {seq_length}")
    print(f"  - íŒ¨ë”© (ìƒ˜í”Œ 0): ì—†ìŒ")
    print(f"  - íŒ¨ë”© (ìƒ˜í”Œ 1): ìœ„ì¹˜ 18~19")
    
    # ë§ˆìŠ¤í¬ ê²°í•©
    combined_mask = combine_masks(custom_mask, padding_mask)
    
    print(f"\nê²°ê³¼:")
    print(f"  - ê²°í•©ëœ ë§ˆìŠ¤í¬ shape: {combined_mask.shape}")
    
    # íŒ¨ë”© ìœ„ì¹˜ê°€ ëª¨ë“  queryì—ì„œ ì°¨ë‹¨ë˜ëŠ”ì§€ í™•ì¸
    # ìƒ˜í”Œ 1ì˜ ìœ„ì¹˜ 18, 19ëŠ” ëª¨ë“  queryì—ì„œ ì°¨ë‹¨ë˜ì–´ì•¼ í•¨
    sample1_mask = combined_mask[1, 0].cpu()
    padding_blocked = torch.all(sample1_mask[:, 18:] == True)
    
    print(f"  - íŒ¨ë”© ìœ„ì¹˜ ì°¨ë‹¨ ì—¬ë¶€: {padding_blocked}")
    
    if not padding_blocked:
        for i in range(seq_length):
            if not torch.all(sample1_mask[i, 18:] == True):
                print(f"  âš ï¸ Query {i}ì—ì„œ íŒ¨ë”© ì°¸ì¡° ê°€ëŠ¥!")
    
    return combined_mask


def test_additive_mask_conversion():
    """Boolean maskë¥¼ additive maskë¡œ ë³€í™˜ í…ŒìŠ¤íŠ¸"""
    print("\n" + "="*80)
    print("í…ŒìŠ¤íŠ¸ 4: Additive Mask ë³€í™˜ (LLaMA í˜¸í™˜)")
    print("="*80)
    
    batch_size = 2
    seq_length = 10
    summary_positions = (7, 10)
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # Boolean mask ìƒì„±
    bool_mask = create_summary_token_attention_mask_optimized(
        batch_size=batch_size,
        seq_length=seq_length,
        summary_token_positions=summary_positions,
        device=device
    )
    
    # Additive maskë¡œ ë³€í™˜
    additive_mask = convert_mask_to_additive(bool_mask, dtype=torch.float32)
    
    print(f"\në³€í™˜ ê²°ê³¼:")
    print(f"  - Boolean mask shape: {bool_mask.shape}")
    print(f"  - Additive mask shape: {additive_mask.shape}")
    print(f"  - Additive mask dtype: {additive_mask.dtype}")
    
    # ê°’ í™•ì¸
    sample_2d = additive_mask[0, 0].cpu()
    unique_values = torch.unique(sample_2d)
    
    print(f"\n  - Unique values: {unique_values.tolist()}")
    print(f"    (0.0 = ì°¸ì¡° ê°€ëŠ¥, -inf = ì°¨ë‹¨)")
    
    # 0.0ê³¼ -infë§Œ ìˆëŠ”ì§€ í™•ì¸
    is_valid = torch.all((sample_2d == 0.0) | (sample_2d == float('-inf')))
    print(f"  - ìœ íš¨ì„± ê²€ì¦: {is_valid}")
    
    # Booleanê³¼ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
    bool_2d = bool_mask[0, 0].cpu()
    matches = torch.all(
        (bool_2d == False) == (sample_2d == 0.0)
    ) and torch.all(
        (bool_2d == True) == (sample_2d == float('-inf'))
    )
    print(f"  - Boolean maskì™€ ì¼ì¹˜: {matches}")
    
    return additive_mask


def visualize_stage1_vs_stage2():
    """Stage 1ê³¼ Stage 2ì˜ attention mask ë¹„êµ ì‹œê°í™”"""
    print("\n" + "="*80)
    print("í…ŒìŠ¤íŠ¸ 5: Stage 1 vs Stage 2 Attention Mask ì‹œê°í™”")
    print("="*80)
    
    batch_size = 1
    seq_length_stage1 = 20  # Stage 1: í”„ë¡¬í”„íŠ¸ + ì´ë¯¸ì§€ + ìš”ì•½í† í°
    seq_length_stage2 = 15  # Stage 2: ì…ë ¥ í”„ë¡¬í”„íŠ¸ + ìš”ì•½í† í° (ì´ë¯¸ì§€ ëŒ€ì‹  ìš”ì•½)
    
    summary_positions_stage1 = (15, 20)  # Stage 1ì—ì„œ ìš”ì•½ í† í° ìœ„ì¹˜
    summary_positions_stage2 = (10, 15)  # Stage 2ì—ì„œ ìš”ì•½ í† í° ìœ„ì¹˜ (ì´ë¯¸ì§€ ì—†ìŒ)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # Stage 1: ìš”ì•½ í† í°ë¼ë¦¬ ì°¸ì¡° ë¶ˆê°€
    print("\n[Stage 1: ìš”ì•½ í† í° ìƒì„±]")
    print(f"  - ì‹œí€€ìŠ¤ êµ¬ì„±: í”„ë¡¬í”„íŠ¸(0~14) + ìš”ì•½í† í°(15~19)")
    print(f"  - ìš”ì•½ í† í°ë¼ë¦¬ ì„œë¡œ ì°¸ì¡° ë¶ˆê°€ (diagonal ì œì™¸)")
    
    mask_stage1 = create_summary_token_attention_mask_optimized(
        batch_size=batch_size,
        seq_length=seq_length_stage1,
        summary_token_positions=summary_positions_stage1,
        device=device
    )
    
    # Stage 2: ì¼ë°˜ causal mask (ìš”ì•½ í† í°ë¼ë¦¬ ì°¸ì¡° ê°€ëŠ¥)
    print("\n[Stage 2: ë‹µë³€ ìƒì„±]")
    print(f"  - ì‹œí€€ìŠ¤ êµ¬ì„±: ì…ë ¥ í”„ë¡¬í”„íŠ¸(0~9) + ìš”ì•½í† í°(10~14)")
    print(f"  - ìš”ì•½ í† í°ë¼ë¦¬ ì°¸ì¡° ê°€ëŠ¥ (ì¼ë°˜ causal mask)")
    
    # Stage 2ëŠ” ì¼ë°˜ causal maskë§Œ ì‚¬ìš©
    mask_stage2 = torch.triu(
        torch.ones(seq_length_stage2, seq_length_stage2, dtype=torch.bool, device=device),
        diagonal=1
    ).unsqueeze(0).unsqueeze(0)
    
    # ì‹œê°í™” ì¤€ë¹„
    mask1_2d = mask_stage1[0, 0].cpu().numpy()
    mask2_2d = mask_stage2[0, 0].cpu().numpy()
    
    # í”Œë¡¯ ìƒì„±
    fig, axes = plt.subplots(1, 2, figsize=(16, 7))
    
    # Stage 1
    im1 = axes[0].imshow(mask1_2d, cmap='RdYlGn_r', interpolation='nearest', vmin=0, vmax=1)
    axes[0].set_title('Stage 1: ìš”ì•½ í† í° ìƒì„±\n(ìš”ì•½ í† í°ë¼ë¦¬ ì°¸ì¡° ë¶ˆê°€)', fontsize=14, fontweight='bold')
    axes[0].set_xlabel('Key Position', fontsize=12)
    axes[0].set_ylabel('Query Position', fontsize=12)
    
    # ì˜ì—­ í‘œì‹œ
    axes[0].axhline(y=15-0.5, color='blue', linewidth=2, linestyle='--', label='ìš”ì•½ í† í° ì‹œì‘')
    axes[0].axvline(x=15-0.5, color='blue', linewidth=2, linestyle='--')
    axes[0].legend(loc='upper right')
    
    # ìš”ì•½ í† í° ì˜ì—­ ê°•ì¡°
    from matplotlib.patches import Rectangle
    rect1 = Rectangle((14.5, 14.5), 5, 5, linewidth=3, edgecolor='red', facecolor='none')
    axes[0].add_patch(rect1)
    axes[0].text(17, 13, 'ìš”ì•½ í† í° ì˜ì—­\n(off-diagonal ì°¨ë‹¨)', 
                ha='center', va='top', fontsize=10, color='red', fontweight='bold')
    
    plt.colorbar(im1, ax=axes[0], label='0=ì°¸ì¡° ê°€ëŠ¥, 1=ì°¨ë‹¨')
    
    # Stage 2
    im2 = axes[1].imshow(mask2_2d, cmap='RdYlGn_r', interpolation='nearest', vmin=0, vmax=1)
    axes[1].set_title('Stage 2: ë‹µë³€ ìƒì„±\n(ìš”ì•½ í† í°ë¼ë¦¬ ì°¸ì¡° ê°€ëŠ¥)', fontsize=14, fontweight='bold')
    axes[1].set_xlabel('Key Position', fontsize=12)
    axes[1].set_ylabel('Query Position', fontsize=12)
    
    # ì˜ì—­ í‘œì‹œ
    axes[1].axhline(y=10-0.5, color='blue', linewidth=2, linestyle='--', label='ìš”ì•½ í† í° ì‹œì‘')
    axes[1].axvline(x=10-0.5, color='blue', linewidth=2, linestyle='--')
    axes[1].legend(loc='upper right')
    
    # ìš”ì•½ í† í° ì˜ì—­ ê°•ì¡°
    rect2 = Rectangle((9.5, 9.5), 5, 5, linewidth=3, edgecolor='green', facecolor='none')
    axes[1].add_patch(rect2)
    axes[1].text(12, 8, 'ìš”ì•½ í† í° ì˜ì—­\n(causal maskë§Œ)', 
                ha='center', va='top', fontsize=10, color='green', fontweight='bold')
    
    plt.colorbar(im2, ax=axes[1], label='0=ì°¸ì¡° ê°€ëŠ¥, 1=ì°¨ë‹¨')
    
    plt.tight_layout()
    
    # ì €ì¥
    output_dir = Path('/home/deokhyeon/Documents/LLaVA')
    output_path = output_dir / 'attention_mask_stage_comparison.png'
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    print(f"\nì‹œê°í™” ì €ì¥: {output_path}")
    
    plt.close()
    
    # ì°¨ì´ì  ì¶œë ¥
    print("\n[ì£¼ìš” ì°¨ì´ì ]")
    
    # Stage 1: ìš”ì•½ í† í° ì˜ì—­ì˜ off-diagonal í™•ì¸
    summary_region_s1 = mask1_2d[15:20, 15:20]
    off_diag_mask = ~np.eye(5, dtype=bool)
    off_diag_blocked_s1 = np.all(summary_region_s1[off_diag_mask] == 1)
    
    print(f"  - Stage 1 ìš”ì•½ ì˜ì—­ off-diagonal ì°¨ë‹¨: {off_diag_blocked_s1}")
    
    # Stage 2: ìš”ì•½ í† í° ì˜ì—­ì€ ì¼ë°˜ causal mask
    summary_region_s2 = mask2_2d[10:15, 10:15]
    is_lower_triangular = np.all(np.tril(summary_region_s2) == 0)
    
    print(f"  - Stage 2 ìš”ì•½ ì˜ì—­ lower triangular: {is_lower_triangular}")
    
    return mask_stage1, mask_stage2


def visualize_detailed_mask():
    """ìƒì„¸í•œ ë§ˆìŠ¤í¬ ì‹œê°í™” (ê° ì˜ì—­ë³„)"""
    print("\n" + "="*80)
    print("í…ŒìŠ¤íŠ¸ 6: ìƒì„¸ Attention Mask ì‹œê°í™”")
    print("="*80)
    
    batch_size = 1
    prompt_len = 5
    image_len = 10
    summary_len = 5
    seq_length = prompt_len + image_len + summary_len
    
    summary_start = prompt_len + image_len
    summary_end = seq_length
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    mask = create_summary_token_attention_mask_optimized(
        batch_size=batch_size,
        seq_length=seq_length,
        summary_token_positions=(summary_start, summary_end),
        device=device
    )
    
    mask_2d = mask[0, 0].cpu().numpy()
    
    # í”Œë¡¯ ìƒì„±
    fig, ax = plt.subplots(figsize=(12, 10))
    
    im = ax.imshow(mask_2d, cmap='RdYlGn_r', interpolation='nearest', vmin=0, vmax=1)
    ax.set_title('Attention Mask ìƒì„¸ ë¶„ì„\n(Stage 1: ìš”ì•½ í† í° ìƒì„±)', fontsize=16, fontweight='bold')
    ax.set_xlabel('Key Position', fontsize=14)
    ax.set_ylabel('Query Position', fontsize=14)
    
    # ê·¸ë¦¬ë“œ ì¶”ê°€
    ax.set_xticks(np.arange(seq_length))
    ax.set_yticks(np.arange(seq_length))
    ax.grid(which='both', color='gray', linestyle='-', linewidth=0.5, alpha=0.3)
    
    # ì˜ì—­ êµ¬ë¶„ì„ 
    ax.axhline(y=prompt_len-0.5, color='blue', linewidth=2, linestyle='--', alpha=0.7)
    ax.axhline(y=summary_start-0.5, color='purple', linewidth=2, linestyle='--', alpha=0.7)
    ax.axvline(x=prompt_len-0.5, color='blue', linewidth=2, linestyle='--', alpha=0.7)
    ax.axvline(x=summary_start-0.5, color='purple', linewidth=2, linestyle='--', alpha=0.7)
    
    # ë ˆì´ë¸” ì¶”ê°€
    ax.text(prompt_len/2, -1.5, 'Prompt', ha='center', fontsize=12, fontweight='bold', color='blue')
    ax.text(prompt_len + image_len/2, -1.5, 'Image', ha='center', fontsize=12, fontweight='bold', color='purple')
    ax.text(summary_start + summary_len/2, -1.5, 'Summary', ha='center', fontsize=12, fontweight='bold', color='red')
    
    ax.text(-1.5, prompt_len/2, 'Prompt', va='center', rotation=90, fontsize=12, fontweight='bold', color='blue')
    ax.text(-1.5, prompt_len + image_len/2, 'Image', va='center', rotation=90, fontsize=12, fontweight='bold', color='purple')
    ax.text(-1.5, summary_start + summary_len/2, 'Summary', va='center', rotation=90, fontsize=12, fontweight='bold', color='red')
    
    # ìš”ì•½ í† í° ì˜ì—­ ê°•ì¡°
    from matplotlib.patches import Rectangle
    rect = Rectangle((summary_start-0.5, summary_start-0.5), summary_len, summary_len, 
                     linewidth=3, edgecolor='red', facecolor='none')
    ax.add_patch(rect)
    
    # ìƒ‰ìƒë°”
    cbar = plt.colorbar(im, ax=ax)
    cbar.set_label('Attention Mask\n(0=ì°¸ì¡° ê°€ëŠ¥, 1=ì°¨ë‹¨)', fontsize=12)
    
    plt.tight_layout()
    
    # ì €ì¥
    output_dir = Path('/home/deokhyeon/Documents/LLaVA')
    output_path = output_dir / 'attention_mask_detailed.png'
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    print(f"\nìƒì„¸ ì‹œê°í™” ì €ì¥: {output_path}")
    
    plt.close()


def print_summary():
    """í…ŒìŠ¤íŠ¸ ìš”ì•½"""
    print("\n" + "="*80)
    print("ì „ì²´ í…ŒìŠ¤íŠ¸ ìš”ì•½")
    print("="*80)
    
    print("\nâœ… êµ¬í˜„ ì˜ë„ í™•ì¸:")
    print("\n1ì°¨ Forward (ìš”ì•½ í† í° ìƒì„±):")
    print("  âœ“ Causal mask ì ìš© (ë¯¸ë˜ í† í° ì°¸ì¡° ë¶ˆê°€)")
    print("  âœ“ ìš”ì•½ í† í°ë¼ë¦¬ ì„œë¡œ ì°¸ì¡° ë¶ˆê°€ (off-diagonal ì°¨ë‹¨)")
    print("  âœ“ ìš”ì•½ í† í°ì€ ìê¸° ìì‹ ë§Œ ì°¸ì¡° ê°€ëŠ¥ (diagonal í—ˆìš©)")
    print("  âœ“ ìš”ì•½ í† í°ì€ í”„ë¡¬í”„íŠ¸/ì´ë¯¸ì§€ ì°¸ì¡° ê°€ëŠ¥")
    
    print("\n2ì°¨ Forward (ë‹µë³€ ìƒì„±):")
    print("  âœ“ ì¼ë°˜ causal maskë§Œ ì ìš©")
    print("  âœ“ ìš”ì•½ í† í°ë¼ë¦¬ ì°¸ì¡° ê°€ëŠ¥ (lower triangular)")
    print("  âœ“ Padding mask ì ìš© ê°€ëŠ¥")
    
    print("\nğŸ“Š ìƒì„±ëœ ì‹œê°í™” íŒŒì¼:")
    print("  - attention_mask_stage_comparison.png")
    print("  - attention_mask_detailed.png")


def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("="*80)
    print("Attention Mask ë””ë²„ê¹… í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("="*80)
    
    try:
        # í…ŒìŠ¤íŠ¸ 1: ê¸°ë³¸ ë§ˆìŠ¤í¬ ìƒì„±
        mask_basic, mask_optimized, summary_positions = test_basic_mask_creation()
        
        # í…ŒìŠ¤íŠ¸ 2: ë§ˆìŠ¤í¬ ì†ì„± ê²€ì¦
        is_valid = test_mask_properties(mask_optimized, summary_positions)
        
        # í…ŒìŠ¤íŠ¸ 3: ë§ˆìŠ¤í¬ ê²°í•©
        combined_mask = test_mask_combination()
        
        # í…ŒìŠ¤íŠ¸ 4: Additive mask ë³€í™˜
        additive_mask = test_additive_mask_conversion()
        
        # í…ŒìŠ¤íŠ¸ 5: Stage 1 vs Stage 2 ë¹„êµ
        mask_stage1, mask_stage2 = visualize_stage1_vs_stage2()
        
        # í…ŒìŠ¤íŠ¸ 6: ìƒì„¸ ì‹œê°í™”
        visualize_detailed_mask()
        
        # ìš”ì•½
        print_summary()
        
        print("\n" + "="*80)
        if is_valid:
            print("âœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼!")
        else:
            print("âš ï¸ ì¼ë¶€ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨. ìœ„ ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        print("="*80)
        
    except Exception as e:
        print(f"\nâŒ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()
