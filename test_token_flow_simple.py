#!/usr/bin/env python
"""
ê°„ë‹¨í•œ í† í° ì „ë‹¬ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
ë‹¨ì¼ ìƒ˜í”Œë¡œ forward passë¥¼ ì‹¤í–‰í•˜ê³  ê° ë‹¨ê³„ì˜ shapeì„ í™•ì¸í•©ë‹ˆë‹¤.
"""

import os
import sys
import argparse

# GPU ì„¤ì •ì„ ê°€ì¥ ë¨¼ì € (import torch ì „ì—)
def parse_args_early():
    """GPU ì„¤ì •ì„ ìœ„í•´ argsë¥¼ ë¨¼ì € íŒŒì‹±"""
    parser = argparse.ArgumentParser(description="í† í° ì „ë‹¬ ê³¼ì • ê°„ë‹¨ í…ŒìŠ¤íŠ¸")
    parser.add_argument("--gpu", type=int, default=1, help="ì‚¬ìš©í•  GPU ë²ˆí˜¸ (ê¸°ë³¸ê°’: 1)")
    args, _ = parser.parse_known_args()
    return args.gpu

GPU_ID = parse_args_early()
os.environ['CUDA_VISIBLE_DEVICES'] = str(GPU_ID)

import torch
from pathlib import Path

# LLaVA ëª¨ë“ˆ import
sys.path.append(str(Path(__file__).parent))
from llava.model.builder import load_pretrained_model
from llava.mm_utils import get_model_name_from_path
from PIL import Image
import numpy as np


def print_section(title):
    """ì„¹ì…˜ ì œëª© ì¶œë ¥"""
    print("\n" + "="*60)
    print(f"  {title}")
    print("="*60)


def test_token_flow(model_path, model_base=None, image_path=None):
    """í† í° ì „ë‹¬ ê³¼ì • í…ŒìŠ¤íŠ¸"""
    
    print_section("1. ëª¨ë¸ ë¡œë“œ")
    
    model_name = get_model_name_from_path(model_path)
    tokenizer, model, image_processor, context_len = load_pretrained_model(
        model_path=model_path,
        model_base=model_base,
        model_name=model_name,
        device_map='auto'
    )
    print(f"âœ“ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_name}")
    if model_base:
        print(f"  - Base model: {model_base}")
    print(f"  - Device: {model.device} (GPU {GPU_ID})")
    print(f"  - dtype: {model.dtype}")
    
    # ëª¨ë¸ì— tokenizer ì„¤ì • (prepare_inputs_for_summary_generation_batchì—ì„œ í•„ìš”)
    model.tokenizer = tokenizer
    
    # ì„¤ì • í™•ì¸
    print_section("2. í† í° ì••ì¶• ì„¤ì • í™•ì¸")
    use_summary_tokens = getattr(model.config, 'use_summary_tokens', False)
    num_summary_tokens = getattr(model.config, 'num_summary_tokens', 0)
    kmeans_init = getattr(model.config, 'kmeans_init', False)
    use_dual_lora = getattr(model.config, 'use_dual_lora', False)
    
    print(f"âœ“ use_summary_tokens: {use_summary_tokens}")
    print(f"âœ“ num_summary_tokens: {num_summary_tokens}")
    print(f"âœ“ kmeans_init: {kmeans_init}")
    print(f"âœ“ use_dual_lora: {use_dual_lora}")
    
    if not use_summary_tokens:
        print("\nâš ï¸  use_summary_tokens=False")
        print("   two-stage forwardë¥¼ í…ŒìŠ¤íŠ¸í•˜ë ¤ë©´ use_summary_tokens=Trueë¡œ ì„¤ì •í•˜ì„¸ìš”.")
        return
    
    # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ì¤€ë¹„
    print_section("3. í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ì¤€ë¹„")
    
    if image_path and Path(image_path).exists():
        image = Image.open(image_path).convert('RGB')
        print(f"âœ“ ì´ë¯¸ì§€ ë¡œë“œ: {image_path}")
    else:
        # ë”ë¯¸ ì´ë¯¸ì§€ ìƒì„±
        image = Image.fromarray(np.random.randint(0, 255, (336, 336, 3), dtype=np.uint8))
        print("âœ“ ë”ë¯¸ ì´ë¯¸ì§€ ìƒì„± (336x336)")
    
    print(f"  - í¬ê¸°: {image.size}")
    
    # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
    from llava.mm_utils import process_images
    image_tensor = process_images([image], image_processor, model.config)
    if isinstance(image_tensor, list):
        image_tensor = image_tensor[0]
    
    # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
    if image_tensor.dim() == 3:
        image_tensor = image_tensor.unsqueeze(0)
    
    image_tensor = image_tensor.to(model.device, dtype=torch.float16)
    print(f"âœ“ ì „ì²˜ë¦¬ ì™„ë£Œ: {image_tensor.shape}")
    
    # 1st Forward í…ŒìŠ¤íŠ¸
    print_section("4. 1st Forward - ëŒ€í‘œ í† í° ìƒì„±")
    
    with torch.no_grad():
        # ì‹¤ì œ ì‚¬ìš©ë˜ëŠ” ê³ ì • í”„ë¡¬í”„íŠ¸ í™•ì¸
        from llava.constants import SUMMARY_PROMPT
        
        # ì‹¤ì œ í”„ë¡¬í”„íŠ¸ í† í° ê°œìˆ˜ í™•ì¸
        real_prompt_tokens = tokenizer.encode(SUMMARY_PROMPT, add_special_tokens=False)
        num_real_prompt_tokens = len(real_prompt_tokens)
        
        print(f"âœ“ 1st forwardì—ì„œ ì‚¬ìš©ë˜ëŠ” ê³ ì • í”„ë¡¬í”„íŠ¸:")
        print(f"  - í…ìŠ¤íŠ¸: '{SUMMARY_PROMPT}'")
        print(f"  - í† í° ê°œìˆ˜: {num_real_prompt_tokens}")
        print(f"  - í† í° ID: {real_prompt_tokens}")
        print(f"")
        
        # ì…ë ¥ ì¤€ë¹„
        result = model.prepare_inputs_for_summary_generation_batch(
            images=image_tensor,
            image_sizes=None,
            return_attention_mask=True
        )
        
        if len(result) == 3:
            inputs_embeds, summary_positions, attention_mask = result
        else:
            inputs_embeds, summary_positions = result
            attention_mask = None
        
        summary_start, summary_end = summary_positions
        num_summary = summary_end - summary_start
        total_seq_len = inputs_embeds.shape[1]
        
        # í† í° êµ¬ì„± ë¶„ì„
        # [ê³ ì • í”„ë¡¬í”„íŠ¸] + [ì´ë¯¸ì§€ í† í°ë“¤] + [ëŒ€í‘œ í† í°ë“¤]
        num_image_tokens = summary_start - num_real_prompt_tokens
        
        print(f"âœ“ 1st forward ì…ë ¥ êµ¬ì„± (ì •í™•í•œ ë¶„ì„):")
        print(f"  ğŸ“Š ì´ ì‹œí€€ìŠ¤ ê¸¸ì´: {total_seq_len}")
        print(f"  ğŸ“ ê³ ì • í”„ë¡¬í”„íŠ¸: {num_real_prompt_tokens}ê°œ í† í° ('{SUMMARY_PROMPT}')")
        print(f"  ğŸ–¼ï¸  ì´ë¯¸ì§€ í† í°: {num_image_tokens}ê°œ (vision encoder + projector ì¶œë ¥)")
        print(f"  ğŸ¯ ëŒ€í‘œ í† í°: {num_summary}ê°œ (ìœ„ì¹˜ {summary_start}~{summary_end})")
        print(f"  âœ… ê²€ì¦: {num_real_prompt_tokens} + {num_image_tokens} + {num_summary} = {total_seq_len}")
        print(f"")
        
        # ê²€ì¦
        expected_len = num_real_prompt_tokens + num_image_tokens + num_summary
        if total_seq_len == expected_len:
            print(f"  âœ… ì‹œí€€ìŠ¤ êµ¬ì„± ì •í™•í•¨!")
        else:
            print(f"  âš ï¸  ì‹œí€€ìŠ¤ ê¸¸ì´ ë¶ˆì¼ì¹˜! ê¸°ëŒ€: {expected_len}, ì‹¤ì œ: {total_seq_len}")
        print(f"")
        print(f"âœ“ inputs_embeds: {inputs_embeds.shape}")
        print(f"âœ“ attention_mask: {'ì ìš©' if attention_mask is not None else 'ì—†ìŒ'}")
        
        # Forward
        forward_kwargs = {
            'inputs_embeds': inputs_embeds,
            'output_hidden_states': True,
            'return_dict': True
        }
        
        if attention_mask is not None:
            from llava.model.attention_utils import combine_masks, convert_mask_to_additive
            batch_size, seq_len = inputs_embeds.shape[0], inputs_embeds.shape[1]
            padding_mask = torch.ones(batch_size, seq_len, dtype=torch.bool, device=inputs_embeds.device)
            combined_mask = combine_masks(attention_mask, padding_mask)
            additive_mask = convert_mask_to_additive(combined_mask, dtype=inputs_embeds.dtype)
            forward_kwargs['attention_mask'] = additive_mask
        
        outputs = model.model(**forward_kwargs)
        
        # Hidden states ì¶”ì¶œ
        last_hidden_states = outputs.hidden_states[-1]
        summary_hidden_states = model.extract_summary_hidden_states(
            last_hidden_states,
            summary_positions
        )
        
        print(f"âœ“ ì¶œë ¥:")
        print(f"  - last_hidden_states: {last_hidden_states.shape}")
        print(f"  - summary_hidden_states: {summary_hidden_states.shape}")
        print(f"  - mean: {summary_hidden_states.mean().item():.6f}")
        print(f"  - std: {summary_hidden_states.std().item():.6f}")
        
        # ê° í† í° norm
        print(f"âœ“ ëŒ€í‘œ í† í° norm:")
        for i in range(num_summary):
            norm = summary_hidden_states[0, i].norm().item()
            print(f"  í† í°[{i}]: {norm:.4f}")
        
        # ëŒ€í‘œ í† í° ê°„ ìœ ì‚¬ë„ ë¶„ì„
        if num_summary > 1:
            norm_hidden = summary_hidden_states[0] / summary_hidden_states[0].norm(dim=1, keepdim=True)
            similarity_matrix = torch.mm(norm_hidden, norm_hidden.t())
            
            # ë¹„ëŒ€ê°ì„  í‰ê·  ìœ ì‚¬ë„
            mask = ~torch.eye(num_summary, dtype=torch.bool, device=similarity_matrix.device)
            off_diagonal_sim = similarity_matrix[mask].mean().item()
            
            print(f"\nâœ“ ëŒ€í‘œ í† í° ê°„ ë…ë¦½ì„± ê²€ì¦:")
            print(f"  - í‰ê·  cosine similarity (ë¹„ëŒ€ê°): {off_diagonal_sim:.4f}")
            print(f"  - ë²”ìœ„: [{similarity_matrix[mask].min():.4f}, {similarity_matrix[mask].max():.4f}]")
            
            if off_diagonal_sim > 0.9:
                print(f"  âš ï¸  ìœ ì‚¬ë„ê°€ ë„ˆë¬´ ë†’ìŒ! ëŒ€í‘œ í† í°ì´ ë…ë¦½ì ì´ì§€ ì•Šì„ ìˆ˜ ìˆìŒ")
            elif off_diagonal_sim < 0.3:
                print(f"  âœ… ëŒ€í‘œ í† í°ì´ ì˜ ë…ë¦½ë˜ì–´ ìˆìŒ (attention mask ì‘ë™)")
            else:
                print(f"  â„¹ï¸  ì ì ˆí•œ ìœ ì‚¬ë„ (ì–´ëŠì •ë„ ë…ë¦½ì„± ìœ ì§€)")
    
    # 2nd Forward í…ŒìŠ¤íŠ¸
    print_section("5. 2nd Forward - ëŒ€í‘œ í† í°ìœ¼ë¡œ ìƒì„±")
    
    with torch.no_grad():
        # í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸
        from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN
        from llava.mm_utils import tokenizer_image_token
        
        text_prompt = "What is in this image?"
        prompt_with_image = DEFAULT_IMAGE_TOKEN + '\n' + text_prompt
        
        input_ids = tokenizer_image_token(
            prompt_with_image,
            tokenizer,
            IMAGE_TOKEN_INDEX,
            return_tensors='pt'
        ).unsqueeze(0).to(model.device)
        
        print(f"âœ“ í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ êµ¬ì¡°:")
        print(f"  - ì›ë³¸: '{text_prompt}'")
        print(f"  - IMAGE í¬í•¨: '{prompt_with_image}'")
        print(f"  - input_ids shape: {input_ids.shape}")
        print(f"  - input_ids: {input_ids[0].tolist()}")
        
        # IMAGE_TOKEN ìœ„ì¹˜ ë¶„ì„
        image_token_mask = (input_ids == IMAGE_TOKEN_INDEX)
        if image_token_mask.any():
            img_pos = image_token_mask.nonzero(as_tuple=True)[1][0].item()
            print(f"  - IMAGE_TOKEN_INDEX ({IMAGE_TOKEN_INDEX})ê°€ ìœ„ì¹˜ {img_pos}ì— ìˆìŒ")
            
            # í† í°ë³„ ë””ì½”ë”©
            decoded_tokens = []
            for tid in input_ids[0].tolist():
                if tid == IMAGE_TOKEN_INDEX:
                    decoded_tokens.append('<IMAGE>')
                else:
                    decoded_tokens.append(tokenizer.decode([tid]))
            print(f"  - í† í° ë¶„í•´: {decoded_tokens}")
            print(f"  - ì˜ë¯¸: <IMAGE> ìœ„ì¹˜ì— ëŒ€í‘œ í† í° {num_summary}ê°œê°€ ì‚½ì…ë¨")
        print(f"")
        
        # 2nd forward ì…ë ¥ ì¤€ë¹„
        attention_mask_input = torch.ones_like(input_ids, dtype=torch.bool)
        
        _, position_ids, attention_mask_2nd, _, inputs_embeds_2nd, labels_2nd = \
            model.prepare_inputs_with_summary(
                input_ids=input_ids,
                position_ids=None,
                attention_mask=attention_mask_input,
                past_key_values=None,
                labels=None,
                summary_hidden_states=summary_hidden_states,
                image_sizes=None
            )
        
        print(f"âœ“ 2nd forward ì…ë ¥:")
        print(f"  - inputs_embeds_2nd: {inputs_embeds_2nd.shape}")
        print(f"  - mean: {inputs_embeds_2nd.mean().item():.6f}")
        print(f"  - std: {inputs_embeds_2nd.std().item():.6f}")
        
        # ëŒ€í‘œ í† í°ì´ ì‚½ì…ëœ ìœ„ì¹˜ ì°¾ê¸° (IMAGE_TOKEN_INDEX ìœ„ì¹˜)
        image_token_positions = (input_ids == IMAGE_TOKEN_INDEX).nonzero(as_tuple=True)
        if len(image_token_positions[1]) > 0:
            image_token_idx = image_token_positions[1][0].item()
            summary_insert_start = image_token_idx
            summary_insert_end = image_token_idx + summary_hidden_states.shape[1]
            
            print(f"\nâœ“ ëŒ€í‘œ í† í° ì‚½ì… ìœ„ì¹˜ ê²€ì¦:")
            print(f"  - IMAGE_TOKEN ìœ„ì¹˜: {image_token_idx}")
            print(f"  - ëŒ€í‘œ í† í° ì‚½ì… ë²”ìœ„: {summary_insert_start}~{summary_insert_end}")
            print(f"  - ê¸°ëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´: {input_ids.shape[1] - 1 + summary_hidden_states.shape[1]} (í…ìŠ¤íŠ¸-1 + ëŒ€í‘œí† í°)")
            print(f"  - ì‹¤ì œ ì‹œí€€ìŠ¤ ê¸¸ì´: {inputs_embeds_2nd.shape[1]}")
            
            if inputs_embeds_2nd.shape[1] == input_ids.shape[1] - 1 + summary_hidden_states.shape[1]:
                print(f"  âœ… ëŒ€í‘œ í† í°ì´ ì˜¬ë°”ë¥´ê²Œ ì‚½ì…ë¨")
            else:
                print(f"  âš ï¸  ì‹œí€€ìŠ¤ ê¸¸ì´ê°€ ì˜ˆìƒê³¼ ë‹¤ë¦„!")
        
        # Forward
        outputs = model(
            inputs_embeds=inputs_embeds_2nd,
            attention_mask=attention_mask_2nd,
        )
        
        logits = outputs.logits
        
        print(f"\nâœ“ 2nd forward ì¶œë ¥:")
        print(f"  - logits: {logits.shape}")
        
        # ë‹¤ìŒ í† í° ì˜ˆì¸¡
        next_token_logits = logits[0, -1]
        next_token_id = next_token_logits.argmax().item()
        next_token = tokenizer.decode([next_token_id])
        
        print(f"âœ“ ì˜ˆì¸¡ëœ ë‹¤ìŒ í† í°: '{next_token}' (ID: {next_token_id})")
    
    # ìš”ì•½
    print_section("6. í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    
    # ìµœì¢… ê²€ì¦
    print("ğŸ” ìµœì¢… ê²€ì¦:")
    print(f"  1. K-means ì‚¬ìš©: {'âœ…' if kmeans_init else 'âŒ'}")
    print(f"  2. ëŒ€í‘œ í† í° ê°œìˆ˜: {num_summary_tokens}ê°œ")
    print(f"  3. Attention mask ì ìš©: {'âœ…' if attention_mask is not None else 'âŒ'}")
    print(f"  4. Dual LoRA ì‚¬ìš©: {'âœ…' if use_dual_lora else 'âŒ'}")
    
    print("\nâœ… ëª¨ë“  ë‹¨ê³„ ì •ìƒ ì‘ë™")
    print(f"âœ… 1st forward: ì´ë¯¸ì§€ â†’ {num_summary_tokens}ê°œ ëŒ€í‘œ í† í° ìƒì„±")
    print(f"âœ… 2nd forward: ëŒ€í‘œ í† í° + í…ìŠ¤íŠ¸ â†’ ìƒì„±")
    
    # ê¶Œì¥ì‚¬í•­
    print("\nğŸ’¡ ì²´í¬í¬ì¸íŠ¸:")
    if off_diagonal_sim > 0.9:
        print("  âš ï¸  ëŒ€í‘œ í† í° ê°„ ìœ ì‚¬ë„ê°€ ë†’ìŒ â†’ attention mask í™•ì¸ í•„ìš”")
    if summary_hidden_states.std().item() > 1.5:
        print("  âš ï¸  ëŒ€í‘œ í† í° stdê°€ ë†’ìŒ â†’ í•™ìŠµ ì•ˆì •ì„± í™•ì¸ í•„ìš”")
    print("")


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="í† í° ì „ë‹¬ ê³¼ì • ê°„ë‹¨ í…ŒìŠ¤íŠ¸")
    parser.add_argument("--model-path", type=str, default="checkpoints/llava-v1.5-7b-token-compress-kmeans",
                        help="ëª¨ë¸ ê²½ë¡œ (LoRA ì²´í¬í¬ì¸íŠ¸)")
    parser.add_argument("--model-base", type=str, default="checkpoints/llava-v1.5-7b",
                        help="Base ëª¨ë¸ ê²½ë¡œ (LoRA ì‚¬ìš© ì‹œ í•„ìš”)")
    parser.add_argument("--image", type=str, default=None,
                        help="í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ê²½ë¡œ (ì—†ìœ¼ë©´ ë”ë¯¸ ì´ë¯¸ì§€ ì‚¬ìš©)")
    parser.add_argument("--gpu", type=int, default=1,
                        help="ì‚¬ìš©í•  GPU ë²ˆí˜¸ (ê¸°ë³¸ê°’: 1)")
    
    args = parser.parse_args()
    
    print("\n" + "="*60)
    print("  í† í° ì „ë‹¬ ë””ë²„ê¹… í…ŒìŠ¤íŠ¸")
    print("="*60)
    print(f"\nëª¨ë¸: {args.model_path}")
    if args.model_base:
        print(f"Base: {args.model_base}")
    print(f"ì´ë¯¸ì§€: {args.image if args.image else 'ë”ë¯¸ ì´ë¯¸ì§€'}")
    print(f"GPU: {GPU_ID}\n")
    
    try:
        test_token_flow(args.model_path, args.model_base, args.image)
    except Exception as e:
        print(f"\nâŒ ì—ëŸ¬ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())
